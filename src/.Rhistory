library(MASS)
Pima
Pima.te
help(Pima)
help(Pima.te)
colnames(Pima.te)
library(MASS)
library(mclust)
library(cluster)
library(ggplot2)
library(GGally)
library(MASS)
library(mclust)
library(cluster)
library(ggplot2)
library(GGally)
library(devtools)
library(caret)
library(MixGHD)
library(gbm)
library(ROCR)
## Section 1 Introduction
# =======================================
# citation("MASS") citation for MASS
# citation() citation for R
# Load diabetes dataset
pima <- rbind(Pima.tr,Pima.te)
## Section 1 Introduction
# =======================================
# citation("MASS") citation for MASS
# citation() citation for R
# Load diabetes dataset
pima <- rbind(Pima.tr,Pima.te)
pima
## Section 2 Pima Dataset
# =======================================
#pairs(pima, col = (as.numeric(pima$type) ))
ggpairs(pima[,-8], aes(colour = pima$type ))
library(GGally)
install.packages("GGally")
k <- 6
n <- 250
d <- 2
# GenerateCRPMix <- function(n, m = 0, s = 1, )
pieces <- c(runif(k), 0)
remainder <- 1 - pieces[1]
if (k > 1)
for (j in 2:k) {
pieces[j] <- remainder * pieces[j]
remainder <- remainder - pieces[j]
}
pieces[k+1] <- remainder
means <- array(rnorm((k+1)*d, 0, 5), dim=c((k+1),d))
# adjust the variance of the rnorms upwards to make clusters
# more distinguishable, or down to make them less so
vars <- array(1 / rgamma((k+1)*d, 1, rate=.5), dim=c((k+1),d))
trueclass <- sample(1:(k+1) , n , replace=TRUE , prob=pieces)
y <- array(rep(0,n*d), dim=c(n,d))
for (j in 1:n) y[j,] <- rnorm(d, means[trueclass[j],], vars[trueclass[j],])
plot(y)
library(dirichletprocess)
colmeans <- rep(0,d)
colvars <- rep(1,d)
for (j in 1:d) {
colmeans[j] <- mean(y[,j])
colvars[j] <- var(y[,j])
y[,j] <- (y[,j]-colmeans[j]) / sqrt(colvars[j])
}
# Initialize a dirichletprocess object; does not actually fit it!
myFit2 <- DirichletProcessMvnormal(y[,1:2],
#g0Priors=c(0,1,1,1),
alphaPriors = c(2,4))
y
k <- 6
n <- 250
d <- 2
# GenerateCRPMix <- function(n, m = 0, s = 1, )
pieces <- c(runif(k), 0)
remainder <- 1 - pieces[1]
if (k > 1)
for (j in 2:k) {
pieces[j] <- remainder * pieces[j]
remainder <- remainder - pieces[j]
}
pieces[k+1] <- remainder
means <- array(rnorm((k+1)*d, 0, 5), dim=c((k+1),d))
# adjust the variance of the rnorms upwards to make clusters
# more distinguishable, or down to make them less so
vars <- array(1 / rgamma((k+1)*d, 1, rate=.5), dim=c((k+1),d))
trueclass <- sample(1:(k+1) , n , replace=TRUE , prob=pieces)
y <- array(rep(0,n*d), dim=c(n,d))
for (j in 1:n) y[j,] <- rnorm(d, means[trueclass[j],], vars[trueclass[j],])
plot(y)
library(dirichletprocess)
colmeans <- rep(0,d)
colvars <- rep(1,d)
for (j in 1:d) {
colmeans[j] <- mean(y[,j])
colvars[j] <- var(y[,j])
y[,j] <- (y[,j]-colmeans[j]) / sqrt(colvars[j])
}
# Initialize a dirichletprocess object; does not actually fit it!
myFit2 <- DirichletProcessMvnormal(y[,1:2],
#g0Priors=c(0,1,1,1),
alphaPriors = c(2,4))
# Now fit it, with 200 iterations in the MCMC algorithm
myFit2 <- Fit(myFit2, its=200)
plot(myFit2)
# initialize the dirichletprocess object
myFit <- DirichletProcessGaussian(y[,1],
g0Priors=c(0, # mean of our prior on cluster means
1, # cluster means and variances are not assumed independent
# in the prior; for clusters w/ high variance, we allow
# more variance in our prior on the mean of that cluster.
# This param is the ratio of the variance of a cluster to
# variance in our Gaussian prior on the mean of that cluster
1,1), # shape and rate params for the inverse-gamma prior
# on cluster variance
alphaPriors = c(2,4)) # We put a Gamma-distributed prior on what I would
# Scale
pima[,-8] <- scale(pima[,-8])
pima
library(caret)
# run this for every section
set.seed(20)
len_pima <- dim(pima)[1]
ind_train <- sample(1:len_pima, as.integer(len_pima*0.80))
train.n <- pima[ind_train,]
test.n  <- pima[-ind_train,]
train.n[,-8] <- scale(train.n[,-8])
test.n[,-8] <- scale(test.n[,-8])
## boot strapped training sample
# get indexs of all yes values in training
ind.yes <- rownames(train.n[train.n$type == "Yes",])
train.bs <- rbind(train.n,train.n[sample(ind.yes,
size = as.integer(as.numeric(table(train.n$type)[1] - table(train.n$type)[2])),
replace = TRUE),])
train.n$type <- as.numeric(train.n$type == "Yes")
test.n$type <- as.numeric(test.n$type == "Yes")
# checking to see if classes are weighted equally.
table(train.bs$type) # done.
train.bs$type <- as.numeric(train.bs$type == "Yes")
Pima
library(MASS)
library(mclust)
library(cluster)
library(ggplot2)
library(GGally)
library(devtools)
library(caret)
library(MixGHD)
library(gbm)
library(ROCR)
Pima
pima
Pima.te
??Pima.te
pima <- rbind(Pima.tr,Pima.te)
## Section 2 Pima Dataset
# =======================================
#pairs(pima, col = (as.numeric(pima$type) ))
ggpairs(pima[,-8], aes(colour = pima$type ))
scale(pima[,-8]])
scale(pima[,-8])
pima[,-8]
# classes are not even so I am going to do bootstrap
# sampling to create my own training set
set.seed(15)
len_pima <- dim(pima)[1]
ind_train <- sample(1:len_pima, as.integer(len_pima*0.80))
train.n <- pima[ind_train,]
test.n  <- pima[-ind_train,]
train.n
# scaling some stuff
train.n[,-8] <- scale(train.n[,-8])
test.n[,-8] <- scale(test.n[,-8])
## boot strapped training sample
# get indexs of all yes values in training
ind.yes <- rownames(train.n[train.n$type == "Yes",])
train.bs <- rbind(train.n,train.n[sample(ind.yes,
size = as.integer(as.numeric(table(train.n$type)[1] - table(train.n$type)[2])),
replace = TRUE),])
train.n$type <- as.numeric(train.n$type == "Yes")
test.n$type <- as.numeric(test.n$type == "Yes")
# checking to see if classes are weighted equally.
table(train.bs$type) # done.
train.bs$type <- as.numeric(train.bs$type == "Yes")
train.bs$type
help(mclust)
train.bs
## Parsimonious family of gaussian models.
mm <- Mclust(train.bs[,-8])
mm
ggpairs(train.bs[,-8],aes(color = mm$classification))
ggpairs(train.bs[,-8],aes(color = as.factor(mm$classification))
)
ggpairs(train.bs[,-8],   aes(color = as.factor(mm$classification, size = 0.1 )))
help(ggpairs)
ggpairs(train.bs[,-8],   aes(color = as.factor(mm$classification)),
lower = list(size = 0.1)
)
ggpairs(train.bs[,-8],   aes(color = as.factor(mm$classification)),
lower = list( continuous = wrap("points",size = 0.1))
)
ggpairs(train.bs[,-8],   aes(color = as.factor(mm$classification)),
lower = list( continuous = wrap("points",size = 0.5))
)
## Pairs plot for the two groups
pairs <- ggpairs(pima[,-8], aes(colour = pima$type ))
pairs
pairs_clust <- ggpairs(train.bs[,-8],   aes(color = as.factor(mm$classification)),
lower = list( continuous = wrap("points",size = 0.5))
)
pairs_clust
pairs
table(mm$classification,train.bs$type)
train.bs %>% select( npreg, glu, age)
library(dplyr)
train.bs %>% select( npreg, glu, age)
mm_red <- Mclust(train.bs_reduced[,-8])
# lets try and remove  bp, skin, bmi, and ped, because age and this npreg do a good job at splitting them
train.bs_reduced <- train.bs %>% select( npreg, glu, age)
mm_red <- Mclust(train.bs_reduced[,-8])
pairs_clust <- ggpairs(train.bs[,-8],   aes(color = as.factor(mm_red$classification)),
lower = list( continuous = wrap("points",size = 0.5))
)
pairs_clust
mm_red$classification
train.bs_reduced
mm_red <- Mclust(train.bs_reduced)
# lets plot results now
pairs_clust <- ggpairs(train.bs[,-8],   aes(color = as.factor(mm_red$classification)),
lower = list( continuous = wrap("points",size = 0.5))
)
# plotting results now
pairs_clust
table(mm_red$classification,t)
table(mm_red$classification,train.bs$type)
mm_red <- Mclust(train.bs_reduced,G = 2)
mm_red
# lets plot results now
pairs_clust <- ggpairs(train.bs[,-8],   aes(color = as.factor(mm_red$classification)),
lower = list( continuous = wrap("points",size = 0.5))
)
# plotting results now
pairs_clust
# lets plot results now
pairs_clust <- ggpairs(train.bs[,-(2:6)],   aes(color = as.factor(mm_red$classification)),
lower = list( continuous = wrap("points",size = 0.5))
)
# plotting results now
pairs_clust
# lets plot results now
pairs_clust <- ggpairs(train.bs[,-c((2:6),-8)],   aes(color = as.factor(mm_red$classification)),
lower = list( continuous = wrap("points",size = 0.5))
)
# plotting results now
pairs_clust
# lets plot results now
pairs_clust <- ggpairs(train.bs[,-c(2:6,8)],   aes(color = as.factor(mm_red$classification)),
lower = list( continuous = wrap("points",size = 0.5))
)
# plotting results now
pairs_clust
# lets plot results now
pairs_clust <- ggpairs(train.bs[,-c(3:6,8)],   aes(color = as.factor(mm_red$classification)),
lower = list( continuous = wrap("points",size = 0.5))
)
# plotting results now
pairs_clust
# lets check the table
table(mm_red$classification,train.bs$type)
# checking the ARI
adjustedRandIndex(mm_red$classification,train.bs$type)
# lets try and remove  bp, skin, bmi, and ped, because age and this npreg do a good job at splitting them
train.bs_reduced <- train.bs %>% select( npreg, age)
mm_red <- Mclust(train.bs_reduced)
# lets plot results now
pairs_clust <- ggpairs(train.bs[,-8],   aes(color = as.factor(mm_red$classification)),
lower = list( continuous = wrap("points",size = 0.5))
)
# plotting results now
pairs_clust
# again its not really splitting them up well
table(mm_red$classification,train.bs$type)
# lets try and remove  bp, skin, bmi, and ped, because age and this npreg do a good job at splitting them
train.bs_reduced <- train.bs %>% select( npreg, age)
mm_red <- Mclust(train.bs_reduced,G=2)
# lets plot results now
pairs_clust <- ggpairs(train.bs[,-8],   aes(color = as.factor(mm_red$classification)),
lower = list( continuous = wrap("points",size = 0.5))
)
# plotting results now
pairs_clust
# lets plot results now
pairs_clust <- ggpairs(train.bs_reduced,   aes(color = as.factor(mm_red$classification)),
lower = list( continuous = wrap("points",size = 0.5))
)
# plotting results now
pairs_clust
# again its not really splitting them up well
table(mm_red$classification,train.bs$type)
adjustedRandIndex(mm_red$classification,train.bs$type) # pretty bad haha
train.bs_reduced
train.bs_reduced$npreg
